"""
News Agent for War Room Debate

7ë²ˆì§¸ War Room ë©¤ë²„ë¡œ ê¸´ê¸‰ ë‰´ìŠ¤(Grounding API)ì™€ ì¼ë°˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬
ë§¤ë§¤ ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤.

Vote Weight: 10%
Data Sources:
- GroundingSearchLog (ê¸´ê¸‰ ë‰´ìŠ¤)
- NewsArticle (ì¼ë°˜ ë‰´ìŠ¤)
- Gemini 2.0 Flash (ê°ì„± ë¶„ì„)

Author: AI Trading System
Date: 2025-12-21
Updated: 2025-12-27 - Added regulatory and litigation news detection
"""

from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import logging
import json

from backend.database.models import NewsArticle, GroundingSearchLog
from backend.database.repository import get_sync_session
from backend.ai.gemini_client import call_gemini_api

logger = logging.getLogger(__name__)


class NewsAgent:
    """ë‰´ìŠ¤ ê¸°ë°˜ íˆ¬í‘œ Agent (War Room 7th member)"""
    
    def __init__(self):
        self.agent_name = "news"
        self.vote_weight = 0.10  # 10% íˆ¬í‘œê¶Œ
        self.model_name = "gemini-2.0-flash-exp"
    
    async def analyze(self, ticker: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ ë¶„ì„ í›„ íˆ¬í‘œ ê²°ì •
        
        Args:
            ticker: ë¶„ì„í•  í‹°ì»¤ (ì˜ˆ: AAPL)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
        
        Returns:
            {
                "agent": "news",
                "action": "BUY/SELL/HOLD",
                "confidence": 0.0-1.0,
                "reasoning": "...",
                "news_count": int,
                "emergency_count": int,
                "sentiment_score": float
            }
        """
        db = get_sync_session()
        
        try:
            # 1. Emergency News ì¡°íšŒ (ìµœê·¼ 15ì¼)
            cutoff = datetime.now() - timedelta(days=15)
            
            # GroundingSearchLog fields: query (not search_query), no ticker column, search_date (not created_at)
            emergency_news = db.query(GroundingSearchLog)\
                .filter(
                    GroundingSearchLog.query.ilike(f"%{ticker}%"),  # Search in query text
                    GroundingSearchLog.search_date >= cutoff         # Use search_date
                )\
                .order_by(GroundingSearchLog.search_date.desc())\
                .limit(5)\
                .all()
            
            # 2. ì¼ë°˜ ë‰´ìŠ¤ ì¡°íšŒ (ìµœê·¼ 15ì¼) - Phase 20 real-time news
            # Priority: tickers field > title/content search
            recent_news = db.query(NewsArticle)\
                .filter(
                    NewsArticle.published_date >= cutoff
                )\
                .order_by(NewsArticle.published_date.desc())\
                .limit(200)\
                .all()

            # í‹°ì»¤ í•„í„°ë§ (ìš°ì„ ìˆœìœ„: tickers ë°°ì—´ > ì œëª©/ë‚´ìš©)
            ticker_news = []
            for n in recent_news:
                # Check tickers array first (from Phase 20)
                if n.tickers and ticker.upper() in [t.upper() for t in n.tickers]:
                    ticker_news.append(n)
                # Fallback: title/content search
                elif ticker.upper() in n.title.upper() or ticker.upper() in (n.content or '').upper():
                    ticker_news.append(n)

                if len(ticker_news) >= 30:
                    break

            recent_news = ticker_news
            
            # 3. ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
            news_summaries = []
            
            for news in emergency_news:
                # GroundingSearchLog has: query, result_count, estimated_cost
                # No 'urgency' field by default
                news_summaries.append({
                    "type": "EMERGENCY",
                    "urgency": "HIGH",  # Default urgency
                    "content": news.query[:200] if news.query else f"Emergency search for {ticker}"
                })
            
            for article in recent_news:
                # Use Phase 20 sentiment_score if available
                sentiment = article.sentiment_score if hasattr(article, 'sentiment_score') and article.sentiment_score else 0.0

                # Add tags for context (from Phase 20 auto-tagging)
                tags_str = ', '.join(article.tags[:3]) if hasattr(article, 'tags') and article.tags else ''

                news_summaries.append({
                    "type": "REGULAR",
                    "title": article.title,
                    "sentiment": sentiment,  # Phase 20 sentiment
                    "tags": tags_str,        # Phase 20 tags
                    "source": article.source if hasattr(article, 'source') else 'Unknown'
                })
            
            # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ íˆ¬í‘œ
            if not news_summaries:
                logger.info(f"ğŸ“° News Agent: No news found for {ticker}")
                return {
                    "agent": "news",
                    "action": "HOLD",
                    "confidence": 0.5,
                    "reasoning": f"{ticker}ì— ëŒ€í•œ ìµœê·¼ 15ì¼ ë‰´ìŠ¤ ì—†ìŒ (ì¤‘ë¦½ ìœ ì§€)",
                    "news_count": 0,
                    "emergency_count": 0,
                    "sentiment_score": 0.0
                }
            
            # 4. ê·œì œ/ì†Œì†¡ ë‰´ìŠ¤ ê°ì§€
            regulatory_analysis = self._detect_regulatory_litigation(news_summaries)

            # 5. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self._analyze_temporal_trend(news_summaries)

            # 6. Geminië¡œ ê°ì„± ë¶„ì„
            logger.info(f"ğŸ“° News Agent: Analyzing {len(news_summaries)} news for {ticker}")
            sentiment_result = await self._analyze_sentiment(ticker, news_summaries, trend_analysis, regulatory_analysis)

            # 7. íˆ¬í‘œ ê²°ì •
            action, confidence = self._decide_action(
                sentiment_result,
                len(emergency_news),
                len(recent_news),
                trend_analysis,
                regulatory_analysis
            )
            
            # íŠ¸ë Œë“œ ì •ë³´ ì¶”ê°€
            trend_info = ""
            if trend_analysis:
                trend_emoji = "ğŸ“ˆ" if trend_analysis['trend'] == 'IMPROVING' else "ğŸ“‰" if trend_analysis['trend'] == 'DETERIORATING' else "â¡ï¸"
                risk_emoji = "âœ…" if trend_analysis['risk_trajectory'] == 'DECREASING' else "âš ï¸" if trend_analysis['risk_trajectory'] == 'INCREASING' else "â–"
                trend_info = f"""
- ë‰´ìŠ¤ íŠ¸ë Œë“œ: {trend_emoji} {trend_analysis['trend']} (ìµœê·¼ {trend_analysis['sentiment_change']:+.2f})
- ìœ„í—˜ë„ ë°©í–¥: {risk_emoji} {trend_analysis['risk_trajectory']}"""

            # ê·œì œ/ì†Œì†¡ ì •ë³´ ì¶”ê°€
            regulatory_info = ""
            if regulatory_analysis['has_risk']:
                reg_emoji = "âš–ï¸" if regulatory_analysis['litigation_count'] > 0 else "ğŸ“œ"
                regulatory_info = f"""
- {reg_emoji} ê·œì œ/ì†Œì†¡: {regulatory_analysis['severity']} ({regulatory_analysis['litigation_count']}ê±´ ì†Œì†¡, {regulatory_analysis['regulatory_count']}ê±´ ê·œì œ)"""

            reasoning = f"""
ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ ({len(emergency_news)}ê°œ ê¸´ê¸‰ + {len(recent_news)}ê°œ ì¼ë°˜):
- ê°ì„± ì ìˆ˜: {sentiment_result['score']:.2f}
- ê¸ì • ë‰´ìŠ¤: {sentiment_result['positive_count']}ê°œ
- ë¶€ì • ë‰´ìŠ¤: {sentiment_result['negative_count']}ê°œ{trend_info}{regulatory_info}
- ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(sentiment_result['keywords'][:5])}
"""
            
            logger.info(f"ğŸ“° News Agent: {action} (confidence: {confidence:.2f})")
            
            return {
                "agent": "news",
                "action": action,
                "confidence": confidence,
                "reasoning": reasoning.strip(),
                "news_count": len(recent_news),
                "emergency_count": len(emergency_news),
                "sentiment_score": sentiment_result['score']
            }
        
        except Exception as e:
            logger.error(f"âŒ News Agent error: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì¤‘ë¦½ íˆ¬í‘œ
            return {
                "agent": "news",
                "action": "HOLD",
                "confidence": 0.5,
                "reasoning": f"ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                "news_count": 0,
                "emergency_count": 0,
                "sentiment_score": 0.0
            }
        
        finally:
            db.close()
    
    def _analyze_temporal_trend(self, news_summaries: List[Dict]) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„: ë‰´ìŠ¤ ê°ì„±ì´ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ë¶„ì„

        Returns:
            {
                "trend": "IMPROVING|DETERIORATING|STABLE",
                "recent_sentiment": float,  # ìµœê·¼ 3ì¼ í‰ê· 
                "older_sentiment": float,   # 4-15ì¼ í‰ê· 
                "sentiment_change": float,  # ë³€í™”ëŸ‰
                "risk_trajectory": "INCREASING|DECREASING|NEUTRAL"
            }
        """
        from datetime import datetime, timedelta

        now = datetime.now()
        recent_cutoff = now - timedelta(days=3)

        recent_news = []
        older_news = []

        for news in news_summaries:
            if news['type'] == 'EMERGENCY':
                # ê¸´ê¸‰ ë‰´ìŠ¤ëŠ” ìµœê·¼ìœ¼ë¡œ ê°„ì£¼
                recent_news.append(news)
                continue

            # ì¼ë°˜ ë‰´ìŠ¤ëŠ” ë°œí–‰ì¼ í™•ì¸ í•„ìš” (news_summariesì— published_at ì¶”ê°€ í•„ìš”)
            # í˜„ì¬ëŠ” ìˆœì„œ ê¸°ë°˜ìœ¼ë¡œ ì ˆë°˜ ë‚˜ëˆ”
            if len(recent_news) < len(news_summaries) / 2:
                recent_news.append(news)
            else:
                older_news.append(news)

        # ê° ê¸°ê°„ë³„ í‰ê·  ê°ì„± ê³„ì‚°
        recent_sentiment = sum(n.get('sentiment', 0) for n in recent_news) / len(recent_news) if recent_news else 0
        older_sentiment = sum(n.get('sentiment', 0) for n in older_news) / len(older_news) if older_news else 0

        sentiment_change = recent_sentiment - older_sentiment

        # íŠ¸ë Œë“œ íŒì •
        if sentiment_change > 0.2:
            trend = "IMPROVING"
            risk_trajectory = "DECREASING"
        elif sentiment_change < -0.2:
            trend = "DETERIORATING"
            risk_trajectory = "INCREASING"
        else:
            trend = "STABLE"
            risk_trajectory = "NEUTRAL"

        return {
            "trend": trend,
            "recent_sentiment": recent_sentiment,
            "older_sentiment": older_sentiment,
            "sentiment_change": sentiment_change,
            "risk_trajectory": risk_trajectory,
            "recent_count": len(recent_news),
            "older_count": len(older_news)
        }

    def _detect_regulatory_litigation(self, news_summaries: List[Dict]) -> Dict[str, Any]:
        """
        ê·œì œ/ì†Œì†¡ ë‰´ìŠ¤ ê°ì§€

        í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€:
        - ì†Œì†¡: lawsuit, litigation, sued, settlement, class action
        - ê·œì œ: regulation, SEC, FTC, antitrust, investigation, probe

        Returns:
            {
                "has_risk": bool,
                "litigation_count": int,  # ì†Œì†¡ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜
                "regulatory_count": int,  # ê·œì œ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜
                "severity": "CRITICAL|HIGH|MODERATE|LOW",
                "keywords_found": List[str]
            }
        """
        # ì†Œì†¡ ê´€ë ¨ í‚¤ì›Œë“œ
        litigation_keywords = [
            'lawsuit', 'litigation', 'sued', 'settlement', 'class action',
            'ì†Œì†¡', 'ì§‘ë‹¨ì†Œì†¡', 'í•©ì˜ê¸ˆ', 'ë²•ì  ë¶„ìŸ', 'ì†Œì†¡ íŒ¨ì†Œ'
        ]

        # ê·œì œ ê´€ë ¨ í‚¤ì›Œë“œ
        regulatory_keywords = [
            'sec', 'ftc', 'doj', 'antitrust', 'investigation', 'probe',
            'fine', 'penalty', 'violation', 'compliance',
            'ê·œì œ', 'ì¡°ì‚¬', 'ì œì¬', 'ìœ„ë°˜', 'ë²Œê¸ˆ', 'ë‹¹êµ­', 'ê°ì‚¬'
        ]

        litigation_count = 0
        regulatory_count = 0
        keywords_found = []

        for news in news_summaries:
            content = ""
            if news['type'] == 'EMERGENCY':
                content = news.get('content', '').lower()
            else:
                content = news.get('title', '').lower()

            # ì†Œì†¡ í‚¤ì›Œë“œ ê²€ì‚¬
            for keyword in litigation_keywords:
                if keyword.lower() in content:
                    litigation_count += 1
                    if keyword not in keywords_found:
                        keywords_found.append(keyword)
                    break  # í•œ ë‰´ìŠ¤ë‹¹ í•œ ë²ˆë§Œ ì¹´ìš´íŠ¸

            # ê·œì œ í‚¤ì›Œë“œ ê²€ì‚¬
            for keyword in regulatory_keywords:
                if keyword.lower() in content:
                    regulatory_count += 1
                    if keyword not in keywords_found:
                        keywords_found.append(keyword)
                    break

        # ì‹¬ê°ë„ íŒì •
        total_issues = litigation_count + regulatory_count

        if total_issues == 0:
            severity = "NONE"
            has_risk = False
        elif total_issues >= 5 or litigation_count >= 3:
            severity = "CRITICAL"
            has_risk = True
        elif total_issues >= 3 or litigation_count >= 2:
            severity = "HIGH"
            has_risk = True
        elif total_issues >= 2:
            severity = "MODERATE"
            has_risk = True
        else:
            severity = "LOW"
            has_risk = True

        return {
            "has_risk": has_risk,
            "litigation_count": litigation_count,
            "regulatory_count": regulatory_count,
            "severity": severity,
            "keywords_found": keywords_found[:5]  # ìµœëŒ€ 5ê°œë§Œ
        }

    async def _analyze_sentiment(self, ticker: str, news_summaries: List[Dict], trend_analysis: Dict = None, regulatory_analysis: Dict = None) -> Dict[str, Any]:
        """Geminië¡œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (ì‹œê³„ì—´ íŠ¸ë Œë“œ í¬í•¨)"""

        if not news_summaries:
            return {
                'score': 0.0,
                'positive_count': 0,
                'negative_count': 0,
                'keywords': [],
                'trend': None
            }

        trend_context = ""
        if trend_analysis:
            trend_context = f"""

ì‹œê³„ì—´ íŠ¸ë Œë“œ:
- ìµœê·¼ 3ì¼ ê°ì„±: {trend_analysis['recent_sentiment']:.2f}
- 4-15ì¼ ê°ì„±: {trend_analysis['older_sentiment']:.2f}
- ë³€í™” ì¶”ì„¸: {trend_analysis['trend']} ({trend_analysis['sentiment_change']:+.2f})
- ìœ„í—˜ë„ ë°©í–¥: {trend_analysis['risk_trajectory']}
"""

        regulatory_context = ""
        if regulatory_analysis and regulatory_analysis['has_risk']:
            regulatory_context = f"""

ê·œì œ/ì†Œì†¡ ì´ìŠˆ:
- ì‹¬ê°ë„: {regulatory_analysis['severity']}
- ì†Œì†¡ ê±´ìˆ˜: {regulatory_analysis['litigation_count']}
- ê·œì œ ê±´ìˆ˜: {regulatory_analysis['regulatory_count']}
- ë°œê²¬ í‚¤ì›Œë“œ: {', '.join(regulatory_analysis['keywords_found'])}

**ê²½ê³ **: ê·œì œ/ì†Œì†¡ ì´ìŠˆëŠ” ì£¼ê°€ì— ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ê°ì„± ì ìˆ˜ì— ë°˜ì˜í•˜ì„¸ìš”.
"""

        prompt = f"""
ë‹¹ì‹ ì€ {ticker} ì£¼ì‹ì— ëŒ€í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¢…í•© ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì„¸ìš”:

{self._format_news_for_prompt(news_summaries)}
{trend_context}{regulatory_context}

**ì¤‘ìš”**:
1. ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ ê³ ë ¤í•˜ì—¬, ìµœê·¼ ë‰´ìŠ¤ê°€ ê³¼ê±° ëŒ€ë¹„ ê°œì„ ë˜ëŠ”ì§€ ì•…í™”ë˜ëŠ”ì§€ ë°˜ì˜í•˜ì„¸ìš”.
2. ê·œì œ/ì†Œì†¡ ì´ìŠˆëŠ” ì‹¬ê°ë„ì— ë”°ë¼ ê°ì„± ì ìˆ˜ë¥¼ -0.2 ~ -0.5 í•˜í–¥ ì¡°ì •í•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´):
{{
  "score": -1.0 ~ 1.0 (ë¶€ì • ~ ê¸ì •),
  "positive_count": ê¸ì • ë‰´ìŠ¤ ê°œìˆ˜,
  "negative_count": ë¶€ì • ë‰´ìŠ¤ ê°œìˆ˜,
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}}
"""
        
        try:
            # Gemini API í˜¸ì¶œ
            response_text = await call_gemini_api(
                prompt=prompt,
                model_name=self.model_name,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±
            # response_textê°€ "```json\n...\n```" í˜•ì‹ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì •ë¦¬
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            
            return {
                'score': float(result.get('score', 0.0)),
                'positive_count': int(result.get('positive_count', 0)),
                'negative_count': int(result.get('negative_count', 0)),
                'keywords': result.get('keywords', [])
            }
        
        except Exception as e:
            logger.error(f"âŒ Sentiment analysis failed: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ë°˜í™˜
            return {
                'score': 0.0,
                'positive_count': 0,
                'negative_count': 0,
                'keywords': []
            }
    
    def _format_news_for_prompt(self, news_summaries: List[Dict]) -> str:
        """ë‰´ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Phase 20 enhanced)"""
        lines = []
        for i, news in enumerate(news_summaries, 1):
            if news['type'] == 'EMERGENCY':
                lines.append(f"{i}. [ê¸´ê¸‰ {news['urgency']}] {news['content']}")
            else:
                # Include sentiment and tags from Phase 20
                sentiment_emoji = "ğŸ“ˆ" if news.get('sentiment', 0) > 0.3 else "ğŸ“‰" if news.get('sentiment', 0) < -0.3 else "â–"
                tags_info = f" [{news.get('tags', '')}]" if news.get('tags') else ""
                source_info = f" ({news.get('source', 'Unknown')})"

                lines.append(f"{i}. {sentiment_emoji} {news['title']}{tags_info}{source_info}")

        return "\n".join(lines)
    
    def _decide_action(
        self,
        sentiment_result: Dict[str, Any],
        emergency_count: int,
        news_count: int,
        trend_analysis: Dict = None,
        regulatory_analysis: Dict = None
    ) -> tuple[str, float]:
        """ê°ì„± ì ìˆ˜ â†’ ë§¤ë§¤ ê²°ì • (ì‹œê³„ì—´ íŠ¸ë Œë“œ ë° ê·œì œ/ì†Œì†¡ ë°˜ì˜)"""

        score = sentiment_result['score']

        # ê¸´ê¸‰ ë‰´ìŠ¤ê°€ ìˆìœ¼ë©´ confidence ë†’ì„
        urgency_boost = 0.2 if emergency_count > 0 else 0

        # ì‹œê³„ì—´ íŠ¸ë Œë“œ ë°˜ì˜
        trend_boost = 0
        if trend_analysis:
            # IMPROVING: ê¸ì • ë‰´ìŠ¤ ì¦ê°€ â†’ BUY ì‹ í˜¸ ê°•í™”
            # DETERIORATING: ë¶€ì • ë‰´ìŠ¤ ì¦ê°€ â†’ SELL ì‹ í˜¸ ê°•í™”
            if trend_analysis['trend'] == 'IMPROVING':
                trend_boost = 0.1
            elif trend_analysis['trend'] == 'DETERIORATING':
                trend_boost = -0.1

        # ê·œì œ/ì†Œì†¡ ë¦¬ìŠ¤í¬ ë°˜ì˜ (HIGHEST PRIORITY)
        regulatory_penalty = 0
        force_sell = False
        if regulatory_analysis and regulatory_analysis['has_risk']:
            if regulatory_analysis['severity'] == 'CRITICAL':
                # ì‹¬ê°í•œ ê·œì œ/ì†Œì†¡ â†’ ê°•ì œ SELL
                regulatory_penalty = -0.5
                force_sell = True
            elif regulatory_analysis['severity'] == 'HIGH':
                regulatory_penalty = -0.3
            elif regulatory_analysis['severity'] == 'MODERATE':
                regulatory_penalty = -0.2
            else:  # LOW
                regulatory_penalty = -0.1

        adjusted_score = score + trend_boost + regulatory_penalty

        # ê·œì œ/ì†Œì†¡ì´ CRITICALì´ë©´ ë¬´ì¡°ê±´ SELL
        if force_sell:
            action = "SELL"
            confidence = 0.90
        elif adjusted_score > 0.6:
            action = "BUY"
            confidence = min(0.95, abs(adjusted_score) + urgency_boost)
        elif adjusted_score < -0.6:
            action = "SELL"
            confidence = min(0.95, abs(adjusted_score) + urgency_boost)
        else:
            action = "HOLD"
            confidence = 0.5 + abs(adjusted_score) * 0.3

        return action, confidence
